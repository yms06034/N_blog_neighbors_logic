{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b595b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import CREATE_NO_WINDOW\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import *\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pyperclip\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8be093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_css(css_selector, browser):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "def finds_css(css_selector, browser):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find_xpath(xpath, browser):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "def finds_xpath(xpath, browser):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_id(e_id, browser):\n",
    "    return browser.find_element(By.ID, e_id)\n",
    "\n",
    "def find_className(cn, browser):\n",
    "    return browser.find_element(By.CLASS_NAME, cn)\n",
    "def finds_className(cn, browser):\n",
    "    return browser.find_elements(By.CLASS_NAME, cn)\n",
    "\n",
    "def find_linktext(lt, browser):\n",
    "    return browser.find_element(By.LINK_TEXT, lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636c8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no--sandbox')\n",
    "    options.add_argument('no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1080,800')\n",
    "    options.add_argument('incognito')\n",
    "\n",
    "    chrome_service = Service('chromedriver')\n",
    "    chrome_service.creationflags = CREATE_NO_WINDOW\n",
    "    chrome_service = Service(executable_path=\"chromedriver.exe\")\n",
    "    browser = webdriver.Chrome(service=chrome_service, options=options)\n",
    "\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba793b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naver_login(NAVER_ID, NAVER_PW, browser):\n",
    "    browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    browser.implicitly_wait(2)\n",
    "    \n",
    "    input_id = find_id('id', browser)\n",
    "    input_pw = find_id('pw', browser)\n",
    "    \n",
    "    pyperclip.copy(NAVER_ID)\n",
    "    input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    pyperclip.copy(NAVER_PW)\n",
    "    input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    input_pw.send_keys(\"\\n\")\n",
    "    \n",
    "    try:\n",
    "        no_save_btn = find_id('new.dontsave', browser)\n",
    "        no_save_btn.click()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "8d520653",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVER_ID = 'itthere2'\n",
    "NAVER_PW = 'naver1!@L'\n",
    "KW = ['동물원']\n",
    "cmt = '좋은 글 감사합니다~ 잘 보고가요!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c48c71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser()\n",
    "naver_login(NAVER_ID, NAVER_PW, browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee0fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import CREATE_NO_WINDOW\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import *\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pyperclip\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def find_css(css_selector, browser):\n",
    "    return browser.find_element(By.CSS_SELECTOR, css_selector)\n",
    "def finds_css(css_selector, browser):\n",
    "    return browser.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "def find_xpath(xpath, browser):\n",
    "    return browser.find_element(By.XPATH, xpath)\n",
    "def finds_xpath(xpath, browser):\n",
    "    return browser.find_elements(By.XPATH, xpath)\n",
    "\n",
    "def find_id(e_id, browser):\n",
    "    return browser.find_element(By.ID, e_id)\n",
    "\n",
    "def find_className(cn, browser):\n",
    "    return browser.find_element(By.CLASS_NAME, cn)\n",
    "def finds_className(cn, browser):\n",
    "    return browser.find_elements(By.CLASS_NAME, cn)\n",
    "\n",
    "def find_linktext(lt, browser):\n",
    "    return browser.find_element(By.LINK_TEXT, lt)\n",
    "\n",
    "def open_browser():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no--sandbox')\n",
    "    options.add_argument('no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1080,800')\n",
    "    options.add_argument('incognito')\n",
    "\n",
    "    chrome_service = Service('chromedriver')\n",
    "    chrome_service.creationflags = CREATE_NO_WINDOW\n",
    "    chrome_service = Service(executable_path=\"chromedriver.exe\")\n",
    "    browser = webdriver.Chrome(service=chrome_service, options=options)\n",
    "\n",
    "    return browser\n",
    "\n",
    "def naver_login(NAVER_ID, NAVER_PW, browser):\n",
    "    browser.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    browser.implicitly_wait(2)\n",
    "    \n",
    "    input_id = find_id('id', browser)\n",
    "    input_pw = find_id('pw', browser)\n",
    "    \n",
    "    pyperclip.copy(NAVER_ID)\n",
    "    input_id.send_keys(Keys.CONTROL, \"v\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    pyperclip.copy(NAVER_PW)\n",
    "    input_pw.send_keys(Keys.CONTROL, \"v\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    input_pw.send_keys(\"\\n\")\n",
    "    \n",
    "    try:\n",
    "        no_save_btn = find_id('new.dontsave', browser)\n",
    "        no_save_btn.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def start_function(NAVER_ID, NAVER_PW, KW, cmt):\n",
    "    browser = open_browser()\n",
    "    naver_login(NAVER_ID, NAVER_PW, browser)\n",
    "\n",
    "    final_hrefs = []\n",
    "    error_urls = []\n",
    "    names = []\n",
    "    unique_urls = []\n",
    "    unique_id = []\n",
    "\n",
    "    cmtNicks = []\n",
    "    cmt_write_urls = []\n",
    "\n",
    "\n",
    "\n",
    "    p_cmt_write_urls = []\n",
    "    p_cmtNicks = []\n",
    "    p_final_hrefs = []\n",
    "\n",
    "    p_names = []\n",
    "    p_unique_id = []\n",
    "    p_unique_urls = []\n",
    "\n",
    "    for i in range(len(KW)):\n",
    "        final_hrefs.clear()\n",
    "        unique_id.clear()\n",
    "        unique_urls.clear()\n",
    "        names.clear()\n",
    "        cmt_write_urls.clear()\n",
    "\n",
    "        browser.get('https://www.naver.com/')\n",
    "        search_input = find_id('query', browser)\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        pyperclip.copy(KW[i])\n",
    "        search_input.send_keys(Keys.CONTROL, 'v')\n",
    "        search_input.send_keys('\\n')\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        try:\n",
    "            popular_theme_urls = []\n",
    "            popular_theme_title = []\n",
    "            for i in finds_className('r4sadT98BjEoozojXloL.MWgfOk7vW9SZowVJA8yP.fds-comps-keyword-chip.fds-modules-keyword-chip', browser):\n",
    "                popular_theme_urls.append(i.get_attribute(\"href\"))\n",
    "                popular_theme_title.append(i.text)\n",
    "\n",
    "            delete_cafe_idx = [idx for idx, keyword in enumerate(popular_theme_title) if '카페' in keyword]\n",
    "\n",
    "            for a in reversed(delete_cafe_idx):\n",
    "                del popular_theme_urls[a]\n",
    "                del popular_theme_title[a]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(popular_theme_urls) >= 1:\n",
    "            for idx in range(len(popular_theme_urls)):\n",
    "                p_final_hrefs.clear()\n",
    "                p_unique_id.clear()\n",
    "                p_unique_urls.clear()\n",
    "                p_names.clear()\n",
    "                p_cmt_write_urls.clear()\n",
    "\n",
    "                browser.get(popular_theme_urls[idx])\n",
    "\n",
    "                browser.implicitly_wait(2)\n",
    "\n",
    "                last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                while True:\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                    time.sleep(.5)\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "                    time.sleep(.5)\n",
    "\n",
    "                    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                    if new_height == last_height:\n",
    "                        break\n",
    "\n",
    "                    last_height = new_height\n",
    "\n",
    "\n",
    "                soup = BS(browser.page_source, \"html.parser\")\n",
    "                a_hrefs = soup.find_all(class_ = \"r4sadT98BjEoozojXloL fds-comps-right-image-text-title\")\n",
    "\n",
    "                for href in a_hrefs:\n",
    "                    p_final_hrefs.append(href['href'])\n",
    "\n",
    "                p_final_hrefs = list(set(p_final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "                for url in p_final_hrefs:\n",
    "                    if 'naver.com/' in url:\n",
    "                        id_ = url.split('.com/')[1].split('/')[0]\n",
    "                        p_names.append(id_)\n",
    "                        if id_ not in p_unique_id:\n",
    "                            p_unique_id.append(id_)\n",
    "\n",
    "\n",
    "                for id_ in p_unique_id:\n",
    "                    count = 0\n",
    "                    for url in p_final_hrefs:\n",
    "                        if id_ in url:\n",
    "                            count += 1\n",
    "                            if count == 1:\n",
    "                                p_unique_urls.append(url)\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                p_unique_urls = [urls for urls in p_unique_urls if 'cafe' not in urls]\n",
    "\n",
    "                p_cmtNicks = []\n",
    "\n",
    "                len_count = 0\n",
    "\n",
    "                for ix in range(len(p_unique_urls)):\n",
    "                    if len_count == 30:\n",
    "                        break\n",
    "                    print(\"전체 게시글 카운트\", ix)\n",
    "                    print(\"댓글 적은 게시글 카운트\", len_count)\n",
    "                    p_cmtNicks.clear()\n",
    "                    browser.get(p_unique_urls[ix])\n",
    "                    time.sleep(.5)\n",
    "                    try:\n",
    "                        browser.switch_to.frame(\"mainFrame\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                        browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                        browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        nicknames = finds_className('u_cbox_nick', browser)\n",
    "                        my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                        print(my_nickname)\n",
    "\n",
    "                        if nicknames:\n",
    "                            for nc in nicknames:\n",
    "                                p_cmtNicks.append(nc.text)\n",
    "\n",
    "                            if my_nickname in p_cmtNicks:\n",
    "                                print('이미 적은 댓글이 있습니다.')\n",
    "                                continue\n",
    "                            else:\n",
    "                                time.sleep(1)\n",
    "                                cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                                cmt_textarea.send_keys(' ')\n",
    "\n",
    "                                pyperclip.copy(cmt)\n",
    "                                cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                                time.sleep(1)\n",
    "\n",
    "                                commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                                commit_btn.click()\n",
    "\n",
    "                                p_cmt_write_urls.append(browser.current_url)\n",
    "                                time.sleep(3)\n",
    "                                try:\n",
    "                                    alert = browser.switch_to.alert\n",
    "                                    alert.accept()\n",
    "                                except:\n",
    "                                    len_count += 1\n",
    "                                    pass\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "                            cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                            cmt_textarea.send_keys(' ')\n",
    "\n",
    "                            pyperclip.copy(cmt)\n",
    "                            cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                            time.sleep(1)\n",
    "\n",
    "                            commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                            commit_btn.click()\n",
    "\n",
    "                            p_cmt_write_urls.append(browser.current_url)\n",
    "                            time.sleep(3)\n",
    "                            try:\n",
    "                                alert = browser.switch_to.alert\n",
    "                                alert.accept()\n",
    "                            except:\n",
    "                                len_count += 1\n",
    "                                pass\n",
    "                    except Exception as ex:\n",
    "                        print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                        continue\n",
    "\n",
    "\n",
    "                dt = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "                df = pd.DataFrame({\"작성한 ULR\" : p_cmt_write_urls})\n",
    "                df.to_excel(f\"{KW[i]}_{popular_theme_title[idx]}_{dt}.xlsx\", index=False)\n",
    "\n",
    "\n",
    "        else:\n",
    "            browser.implicitly_wait(2)\n",
    "\n",
    "            find_linktext('VIEW', browser).click()\n",
    "\n",
    "            browser.implicitly_wait(2)\n",
    "\n",
    "            find_linktext('블로그', browser).click()\n",
    "\n",
    "            browser.implicitly_wait(2)\n",
    "\n",
    "            last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                time.sleep(.5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "                time.sleep(.5)\n",
    "\n",
    "                new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "\n",
    "                last_height = new_height\n",
    "\n",
    "\n",
    "            soup = BS(browser.page_source, \"html.parser\")\n",
    "            a_hrefs = soup.find_all(class_ = \"title_link\")\n",
    "\n",
    "            for href in a_hrefs:\n",
    "                final_hrefs.append(href['href'])\n",
    "\n",
    "            final_hrefs = list(set(final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "            for url in final_hrefs:\n",
    "                if 'naver.com/' in url:\n",
    "                    id_ = url.split('.com/')[1].split('/')[0]\n",
    "                    names.append(id_)\n",
    "                    if id_ not in unique_id:\n",
    "                        unique_id.append(id_)\n",
    "\n",
    "\n",
    "            for id_ in unique_id:\n",
    "                count = 0\n",
    "                for url in final_hrefs:\n",
    "                    if id_ in url:\n",
    "                        count += 1\n",
    "                        if count == 1:\n",
    "                            unique_urls.append(url)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            len_count = 0\n",
    "            for i in range(len(unique_urls)):\n",
    "                cmtNicks.clear()\n",
    "                browser.get(unique_urls[i])\n",
    "                time.sleep(.5)\n",
    "\n",
    "                try:\n",
    "                    try:\n",
    "                        browser.switch_to.frame(\"mainFrame\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    nicknames = finds_className('u_cbox_nick', browser)\n",
    "                    my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                    print(my_nickname)\n",
    "\n",
    "                    if nicknames:\n",
    "                        for nc in nicknames:\n",
    "                            cmtNicks.append(nc.text)\n",
    "\n",
    "                        if my_nickname in cmtNicks:\n",
    "                            continue\n",
    "                        else:\n",
    "                            time.sleep(1.5)\n",
    "                            cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                            cmt_textarea.send_keys(' ')\n",
    "\n",
    "                            pyperclip.copy(cmt)\n",
    "                            cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                            time.sleep(1)\n",
    "\n",
    "                            commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                            commit_btn.click()\n",
    "\n",
    "                            cmt_write_urls.append(browser.current_url)\n",
    "                            time.sleep(3)\n",
    "                            try:\n",
    "                                alert = browser.switch_to.alert\n",
    "                                alert.accept()\n",
    "                            except:\n",
    "                                len_count += 1\n",
    "                                pass\n",
    "                    else:\n",
    "                        time.sleep(1)\n",
    "                        cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                        cmt_textarea.send_keys(' ')\n",
    "\n",
    "                        pyperclip.copy(cmt)\n",
    "                        cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                        commit_btn.click()\n",
    "\n",
    "                        cmt_write_urls.append(browser.current_url)\n",
    "                        time.sleep(3)\n",
    "                        try:\n",
    "                            alert = browser.switch_to.alert\n",
    "                            alert.accept()\n",
    "                        except:\n",
    "                            len_count += 1\n",
    "                            pass\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                    continue\n",
    "\n",
    "            dt = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "            df = pd.DataFrame({\"작성한 ULR\" : cmt_write_urls})\n",
    "            df.to_excel(f\"{KW[i]}_{dt}.xlsx\", index=False)\n",
    "\n",
    "    return cmt_write_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af00431",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser()\n",
    "naver_login(NAVER_ID, NAVER_PW, browser)\n",
    "\n",
    "final_hrefs = []\n",
    "error_urls = []\n",
    "names = []\n",
    "unique_urls = []\n",
    "unique_id = []\n",
    "\n",
    "cmtNicks = []\n",
    "cmt_write_urls = []\n",
    "\n",
    "\n",
    "\n",
    "p_cmt_write_urls = []\n",
    "p_cmtNicks = []\n",
    "p_final_hrefs = []\n",
    "\n",
    "p_names = []\n",
    "p_unique_id = []\n",
    "p_unique_urls = []\n",
    "\n",
    "for i in range(len(KW)):\n",
    "    final_hrefs.clear()\n",
    "    unique_id.clear()\n",
    "    unique_urls.clear()\n",
    "    names.clear()\n",
    "    cmt_write_urls.clear()\n",
    "\n",
    "    browser.get('https://www.naver.com/')\n",
    "    search_input = find_id('query', browser)\n",
    "\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    pyperclip.copy(KW[i])\n",
    "    search_input.send_keys(Keys.CONTROL, 'v')\n",
    "    search_input.send_keys('\\n')\n",
    "\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    try:\n",
    "        popular_theme_urls = []\n",
    "        popular_theme_title = []\n",
    "        for i in finds_className('r4sadT98BjEoozojXloL.MWgfOk7vW9SZowVJA8yP.fds-comps-keyword-chip.fds-modules-keyword-chip', browser):\n",
    "            popular_theme_urls.append(i.get_attribute(\"href\"))\n",
    "            popular_theme_title.append(i.text)\n",
    "\n",
    "        delete_cafe_idx = [idx for idx, keyword in enumerate(popular_theme_title) if '카페' in keyword]\n",
    "\n",
    "        for a in reversed(delete_cafe_idx):\n",
    "            del popular_theme_urls[a]\n",
    "            del popular_theme_title[a]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if len(popular_theme_urls) >= 1:\n",
    "        for idx in range(len(popular_theme_urls)):\n",
    "            p_final_hrefs.clear()\n",
    "            p_unique_id.clear()\n",
    "            p_unique_urls.clear()\n",
    "            p_names.clear()\n",
    "            p_cmt_write_urls.clear()\n",
    "\n",
    "            browser.get(popular_theme_urls[idx])\n",
    "\n",
    "            browser.implicitly_wait(2)\n",
    "\n",
    "            last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                time.sleep(.5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "                time.sleep(.5)\n",
    "\n",
    "                new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "\n",
    "                last_height = new_height\n",
    "\n",
    "\n",
    "            soup = BS(browser.page_source, \"html.parser\")\n",
    "            a_hrefs = soup.find_all(class_ = \"r4sadT98BjEoozojXloL fds-comps-right-image-text-title\")\n",
    "\n",
    "            for href in a_hrefs:\n",
    "                p_final_hrefs.append(href['href'])\n",
    "\n",
    "            p_final_hrefs = list(set(p_final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "            for url in p_final_hrefs:\n",
    "                if 'naver.com/' in url:\n",
    "                    id_ = url.split('.com/')[1].split('/')[0]\n",
    "                    p_names.append(id_)\n",
    "                    if id_ not in p_unique_id:\n",
    "                        p_unique_id.append(id_)\n",
    "\n",
    "\n",
    "            for id_ in p_unique_id:\n",
    "                count = 0\n",
    "                for url in p_final_hrefs:\n",
    "                    if id_ in url:\n",
    "                        count += 1\n",
    "                        if count == 1:\n",
    "                            p_unique_urls.append(url)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            p_unique_urls = [urls for urls in p_unique_urls if 'cafe' not in urls]\n",
    "\n",
    "            p_cmtNicks = []\n",
    "\n",
    "            len_count = 0\n",
    "\n",
    "            for ix in range(len(p_unique_urls)):\n",
    "                if len_count == 30:\n",
    "                    break\n",
    "                    \n",
    "                r_select = random.choice(current_lists)\n",
    "                cmt = r_select[0]\n",
    "                \n",
    "                p_cmtNicks.clear()\n",
    "                browser.get(p_unique_urls[ix])\n",
    "                time.sleep(.5)\n",
    "                try:\n",
    "                    browser.switch_to.frame(\"mainFrame\")\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    nicknames = finds_className('u_cbox_nick', browser)\n",
    "                    my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                    print(my_nickname)\n",
    "                    \n",
    "                    soup = BS(browser.page_source, \"html.parser\")\n",
    "                    n_a_hrefs = soup.find_all(class_ = \"u_cbox_name\")\n",
    "                    print(my_nickname)\n",
    "\n",
    "                    n_n_list = []\n",
    "\n",
    "                    for n_href in n_a_hrefs:\n",
    "                        n_n_list.append(n_href['href'])\n",
    "\n",
    "                    n_n_list = list(set(n_n_list))\n",
    "\n",
    "                    if nicknames:\n",
    "                        for nc in nicknames:\n",
    "                            p_cmtNicks.append(nc.text)\n",
    "\n",
    "                        if my_nickname in p_cmtNicks:\n",
    "                            print('이미 적은 댓글이 있습니다.')\n",
    "                            continue\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "                            cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                            cmt_textarea.send_keys(' ')\n",
    "\n",
    "                            pyperclip.copy(cmt)\n",
    "                            cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                            time.sleep(1)\n",
    "\n",
    "                            commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                            commit_btn.click()\n",
    "\n",
    "                            p_cmt_write_urls.append(browser.current_url)\n",
    "                            time.sleep(3)\n",
    "                            try:\n",
    "                                alert = browser.switch_to.alert\n",
    "                                alert.accept()\n",
    "                            except:\n",
    "                                len_count += 1\n",
    "                                pass\n",
    "                    else:\n",
    "                        time.sleep(1)\n",
    "                        cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                        cmt_textarea.send_keys(' ')\n",
    "\n",
    "                        pyperclip.copy(cmt)\n",
    "                        cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                        commit_btn.click()\n",
    "\n",
    "                        p_cmt_write_urls.append(browser.current_url)\n",
    "                        time.sleep(3)\n",
    "                        try:\n",
    "                            alert = browser.switch_to.alert\n",
    "                            alert.accept()\n",
    "                        except:\n",
    "                            len_count += 1\n",
    "                            pass\n",
    "                except Exception as ex:\n",
    "                    print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                    continue\n",
    "\n",
    "\n",
    "            dt = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "            df = pd.DataFrame({\"작성한 ULR\" : p_cmt_write_urls})\n",
    "            df.to_excel(f\"{KW[i]}_{popular_theme_title[idx]}_{dt}.xlsx\", index=False)\n",
    "\n",
    "\n",
    "    else:\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        find_linktext('VIEW', browser).click()\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        find_linktext('블로그', browser).click()\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            time.sleep(.5)\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "            time.sleep(.5)\n",
    "\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        a_hrefs = soup.find_all(class_ = \"title_link\")\n",
    "\n",
    "        for href in a_hrefs:\n",
    "            final_hrefs.append(href['href'])\n",
    "\n",
    "        final_hrefs = list(set(final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "        for url in final_hrefs:\n",
    "            if 'naver.com/' in url:\n",
    "                id_ = url.split('.com/')[1].split('/')[0]\n",
    "                names.append(id_)\n",
    "                if id_ not in unique_id:\n",
    "                    unique_id.append(id_)\n",
    "\n",
    "\n",
    "        for id_ in unique_id:\n",
    "            count = 0\n",
    "            for url in final_hrefs:\n",
    "                if id_ in url:\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        unique_urls.append(url)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        len_count = 0\n",
    "        for i in range(len(unique_urls)):\n",
    "            if len_count == 50:\n",
    "                break\n",
    "            r_select = random.choice(current_lists)\n",
    "            cmt = r_select[0]\n",
    "            \n",
    "            cmtNicks.clear()\n",
    "            browser.get(unique_urls[i])\n",
    "            time.sleep(.5)\n",
    "\n",
    "            try:\n",
    "                browser.switch_to.frame(\"mainFrame\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "\n",
    "                a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                time.sleep(1)\n",
    "\n",
    "                a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                nicknames = finds_className('u_cbox_nick', browser)\n",
    "                my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                print(my_nickname)\n",
    "                \n",
    "                soup = BS(browser.page_source, \"html.parser\")\n",
    "                n_a_hrefs = soup.find_all(class_ = \"u_cbox_name\")\n",
    "                print(my_nickname)\n",
    "\n",
    "                n_n_list = []\n",
    "\n",
    "                for n_href in n_a_hrefs:\n",
    "                    n_n_list.append(n_href['href'])\n",
    "\n",
    "                n_n_list = list(set(n_n_list))\n",
    "\n",
    "                if nicknames:\n",
    "                    for nc in nicknames:\n",
    "                        cmtNicks.append(nc.text)\n",
    "\n",
    "                    if my_nickname in cmtNicks:\n",
    "                        continue\n",
    "                    else:\n",
    "                        time.sleep(1.5)\n",
    "                        cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                        cmt_textarea.send_keys(' ')\n",
    "\n",
    "                        pyperclip.copy(cmt)\n",
    "                        cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                        commit_btn.click()\n",
    "\n",
    "                        cmt_write_urls.append(browser.current_url)\n",
    "                        time.sleep(3)\n",
    "                        try:\n",
    "                            alert = browser.switch_to.alert\n",
    "                            alert.accept()\n",
    "                        except:\n",
    "                            len_count += 1\n",
    "                            pass\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "                    cmt_textarea = find_className('u_cbox_text_mention', browser)\n",
    "\n",
    "                    cmt_textarea.send_keys(' ')\n",
    "\n",
    "                    pyperclip.copy(cmt)\n",
    "                    cmt_textarea.send_keys(Keys.CONTROL, 'v')\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    commit_btn = find_css('div.u_cbox_upload > button.u_cbox_btn_upload', browser)\n",
    "                    commit_btn.click()\n",
    "\n",
    "                    cmt_write_urls.append(browser.current_url)\n",
    "                    time.sleep(3)\n",
    "                    try:\n",
    "                        alert = browser.switch_to.alert\n",
    "                        alert.accept()\n",
    "                    except:\n",
    "                        len_count += 1\n",
    "                        pass\n",
    "\n",
    "            except Exception as ex:\n",
    "                print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                continue\n",
    "\n",
    "        dt = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "        df = pd.DataFrame({\"작성한 ULR\" : cmt_write_urls})\n",
    "        df.to_excel(f\"{KW[i]}_{dt}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "18bd35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser()\n",
    "naver_login(NAVER_ID, NAVER_PW, browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "42886fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원고 리스트\n",
    "current_lists =[['TEST1'],['TEST2'],['TEST3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1f4937c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST2\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 1\n",
      "TEST1\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 13\n",
      "TEST3\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 22\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 38\n",
      "TEST1\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 38\n",
      "TEST2\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 38\n",
      "TEST3\n",
      "1\n",
      "댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.\n",
      "TEST3\n",
      "2\n",
      "댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.\n",
      "TEST3\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 38\n",
      "TEST2\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 38\n",
      "TEST3\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 68\n",
      "TEST1\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 74\n",
      "TEST2\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 74\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 98\n",
      "TEST2\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 100\n",
      "TEST3\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 107\n",
      "TEST3\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 125\n",
      "TEST3\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 126\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 126\n",
      "TEST1\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 135\n",
      "TEST2\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 136\n",
      "TEST2\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 138\n",
      "TEST1\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 143\n",
      "TEST3\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 146\n",
      "TEST2\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 146\n",
      "TEST3\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 149\n",
      "TEST1\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 151\n",
      "TEST3\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 156\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 156\n",
      "TEST2\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 164\n",
      "TEST1\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 166\n",
      "TEST2\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 174\n",
      "TEST3\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 174\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 182\n",
      "TEST3\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 182\n",
      "TEST2\n",
      "0\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 185\n",
      "TEST2\n",
      "1\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 190\n",
      "TEST1\n",
      "2\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 190\n",
      "TEST2\n",
      "3\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 190\n",
      "TEST1\n",
      "4\n",
      "인포매니아\n",
      "인포매니아\n",
      "들어갈 URL : 194\n"
     ]
    }
   ],
   "source": [
    "browser = open_browser()\n",
    "naver_login(NAVER_ID, NAVER_PW, browser)\n",
    "\n",
    "final_hrefs = []\n",
    "error_urls = []\n",
    "names = []\n",
    "unique_urls = []\n",
    "unique_id = []\n",
    "\n",
    "cmtNicks = []\n",
    "cmt_write_urls = []\n",
    "\n",
    "\n",
    "\n",
    "p_cmt_write_urls = []\n",
    "p_cmtNicks = []\n",
    "p_final_hrefs = []\n",
    "\n",
    "p_names = []\n",
    "p_unique_id = []\n",
    "p_unique_urls = []\n",
    "\n",
    "n_n_list = []\n",
    "\n",
    "for i in range(len(KW)):\n",
    "    final_hrefs.clear()\n",
    "    unique_id.clear()\n",
    "    unique_urls.clear()\n",
    "    names.clear()\n",
    "    cmt_write_urls.clear()\n",
    "    n_n_list.clear()\n",
    "\n",
    "    browser.get('https://www.naver.com/')\n",
    "    search_input = find_id('query', browser)\n",
    "\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    pyperclip.copy(KW[i])\n",
    "    search_input.send_keys(Keys.CONTROL, 'v')\n",
    "    search_input.send_keys('\\n')\n",
    "\n",
    "    browser.implicitly_wait(2)\n",
    "\n",
    "    try:\n",
    "        popular_theme_urls = []\n",
    "        popular_theme_title = []\n",
    "        for i in finds_className('r4sadT98BjEoozojXloL.MWgfOk7vW9SZowVJA8yP.fds-comps-keyword-chip.fds-modules-keyword-chip', browser):\n",
    "            popular_theme_urls.append(i.get_attribute(\"href\"))\n",
    "            popular_theme_title.append(i.text)\n",
    "\n",
    "        delete_cafe_idx = [idx for idx, keyword in enumerate(popular_theme_title) if '카페' in keyword]\n",
    "\n",
    "        for a in reversed(delete_cafe_idx):\n",
    "            del popular_theme_urls[a]\n",
    "            del popular_theme_title[a]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if len(popular_theme_urls) >= 1:\n",
    "        for idx in range(len(popular_theme_urls)):\n",
    "            p_final_hrefs.clear()\n",
    "            p_unique_id.clear()\n",
    "            p_unique_urls.clear()\n",
    "            p_names.clear()\n",
    "            p_cmt_write_urls.clear()\n",
    "\n",
    "            browser.get(popular_theme_urls[idx])\n",
    "\n",
    "            browser.implicitly_wait(2)\n",
    "\n",
    "            last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                time.sleep(.5)\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "                time.sleep(.5)\n",
    "\n",
    "                new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "\n",
    "                last_height = new_height\n",
    "\n",
    "\n",
    "            soup = BS(browser.page_source, \"html.parser\")\n",
    "            a_hrefs = soup.find_all(class_ = \"r4sadT98BjEoozojXloL fds-comps-right-image-text-title\")\n",
    "\n",
    "            for href in a_hrefs:\n",
    "                p_final_hrefs.append(href['href'])\n",
    "\n",
    "            p_final_hrefs = list(set(p_final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "            for url in p_final_hrefs:\n",
    "                if 'naver.com/' in url:\n",
    "                    id_ = url.split('.com/')[1].split('/')[0]\n",
    "                    p_names.append(id_)\n",
    "                    if id_ not in p_unique_id:\n",
    "                        p_unique_id.append(id_)\n",
    "\n",
    "\n",
    "            for id_ in p_unique_id:\n",
    "                count = 0\n",
    "                for url in p_final_hrefs:\n",
    "                    if id_ in url:\n",
    "                        count += 1\n",
    "                        if count == 1:\n",
    "                            p_unique_urls.append(url)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            p_unique_urls = [urls for urls in p_unique_urls if 'cafe' not in urls]\n",
    "\n",
    "            p_cmtNicks = []\n",
    "\n",
    "            len_count = 0\n",
    "\n",
    "            for ix in range(len(p_unique_urls)):\n",
    "                if len_count == 5:\n",
    "                    break\n",
    "                    \n",
    "                r_select = random.choice(current_lists)\n",
    "                cmt = r_select[0]\n",
    "                print(cmt)\n",
    "                \n",
    "                p_cmtNicks.clear()\n",
    "                browser.get(p_unique_urls[ix])\n",
    "                time.sleep(.5)\n",
    "                print(len_count)\n",
    "                len_count += 1\n",
    "                try:\n",
    "                    browser.switch_to.frame(\"mainFrame\")\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                    browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    nicknames = finds_className('u_cbox_nick', browser)\n",
    "                    my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                    print(my_nickname)\n",
    "                    \n",
    "                    soup = BS(browser.page_source, \"html.parser\")\n",
    "                    n_a_hrefs = soup.find_all(class_ = \"u_cbox_name\")\n",
    "\n",
    "                    for n_href in n_a_hrefs:\n",
    "                        n_n_list.append(n_href['href'])\n",
    "\n",
    "                    n_n_list = list(set(n_n_list))\n",
    "                    print(f'들어갈 URL : {len(n_n_list)}')\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                    continue\n",
    "\n",
    "\n",
    "    else:\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        find_linktext('VIEW', browser).click()\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        find_linktext('블로그', browser).click()\n",
    "\n",
    "        browser.implicitly_wait(2)\n",
    "\n",
    "        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            time.sleep(.5)\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "            time.sleep(.5)\n",
    "\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "        soup = BS(browser.page_source, \"html.parser\")\n",
    "        a_hrefs = soup.find_all(class_ = \"title_link\")\n",
    "\n",
    "        for href in a_hrefs:\n",
    "            final_hrefs.append(href['href'])\n",
    "\n",
    "        final_hrefs = list(set(final_hrefs))\n",
    "\n",
    "\n",
    "\n",
    "        for url in final_hrefs:\n",
    "            if 'naver.com/' in url:\n",
    "                id_ = url.split('.com/')[1].split('/')[0]\n",
    "                names.append(id_)\n",
    "                if id_ not in unique_id:\n",
    "                    unique_id.append(id_)\n",
    "\n",
    "\n",
    "        for id_ in unique_id:\n",
    "            count = 0\n",
    "            for url in final_hrefs:\n",
    "                if id_ in url:\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        unique_urls.append(url)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        len_count = 0\n",
    "        for i in range(len(unique_urls)):\n",
    "            if len_count == 5:\n",
    "                break\n",
    "            r_select = random.choice(current_lists)\n",
    "            cmt = r_select[0]\n",
    "            print(cmt)\n",
    "            \n",
    "            cmtNicks.clear()\n",
    "            browser.get(unique_urls[i])\n",
    "            time.sleep(.5)\n",
    "            \n",
    "            print(len_count)\n",
    "            len_count += 1\n",
    "\n",
    "            try:\n",
    "                browser.switch_to.frame(\"mainFrame\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "\n",
    "                a_test = find_css('div.area_comment.pcol2 > a', browser)\n",
    "                browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "                time.sleep(1)\n",
    "\n",
    "                a_test = find_css('span.u_cbox_secret_tag > input', browser)\n",
    "                browser.execute_script(\"arguments[0].click();\", a_test)\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "                nicknames = finds_className('u_cbox_nick', browser)\n",
    "                my_nickname = find_className('u_cbox_write_name', browser).text\n",
    "                print(my_nickname)\n",
    "                \n",
    "                soup = BS(browser.page_source, \"html.parser\")\n",
    "                n_a_hrefs = soup.find_all(class_ = \"u_cbox_name\")\n",
    "\n",
    "                for n_href in n_a_hrefs:\n",
    "                    n_n_list.append(n_href['href'])\n",
    "\n",
    "                n_n_list = list(set(n_n_list))\n",
    "                print(f'들어갈 URL : {len(n_n_list)}')\n",
    "\n",
    "            except Exception as ex:\n",
    "                print('댓글 및 경고창의 이유로 댓글을 적을 수 없습니다.')\n",
    "                continue\n",
    "                \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "280e1d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://blog.naver.com/gngn430',\n",
       " 'https://blog.naver.com/thdwodyd0526',\n",
       " 'https://blog.naver.com/kimgyounghwa',\n",
       " 'https://blog.naver.com/1025thddl',\n",
       " 'https://blog.naver.com/nw48666',\n",
       " 'https://blog.naver.com/ii2ppu2ii5',\n",
       " 'https://blog.naver.com/tkatlrltkatl',\n",
       " 'https://blog.naver.com/nnuuuuuna',\n",
       " 'https://blog.naver.com/yeonem',\n",
       " 'https://blog.naver.com/ssoonn_2',\n",
       " 'https://blog.naver.com/pemares',\n",
       " 'https://blog.naver.com/dowls9716',\n",
       " 'https://blog.naver.com/ddabini1',\n",
       " 'https://blog.naver.com/thwlfh81',\n",
       " 'https://blog.naver.com/kjylove3535',\n",
       " 'https://blog.naver.com/jihyun316',\n",
       " 'https://blog.naver.com/hhjjm8',\n",
       " 'https://blog.naver.com/trigemcho',\n",
       " 'https://blog.naver.com/hdsxn7339',\n",
       " 'https://blog.naver.com/sooeunandgod',\n",
       " 'https://blog.naver.com/hiokay2025',\n",
       " 'https://blog.naver.com/hjh8808',\n",
       " 'https://blog.naver.com/pyr5267',\n",
       " 'https://blog.naver.com/myblog_jin',\n",
       " 'https://blog.naver.com/minhs24-s',\n",
       " 'https://blog.naver.com/kimbbang486',\n",
       " 'https://blog.naver.com/clsrn9606',\n",
       " 'https://blog.naver.com/cmm_motors',\n",
       " 'https://blog.naver.com/eoqja94',\n",
       " 'https://blog.naver.com/blooming_stellaa',\n",
       " 'https://blog.naver.com/ddobokmam',\n",
       " 'https://blog.naver.com/doo4200',\n",
       " 'https://blog.naver.com/ehdtn2581',\n",
       " 'https://blog.naver.com/htw8352',\n",
       " 'https://blog.naver.com/hyunha2002',\n",
       " 'https://blog.naver.com/kimuj2004',\n",
       " 'https://blog.naver.com/i486you',\n",
       " 'https://blog.naver.com/balmershj',\n",
       " 'https://blog.naver.com/kite1102',\n",
       " 'https://blog.naver.com/jesh0110',\n",
       " 'https://blog.naver.com/w00kuck',\n",
       " 'https://blog.naver.com/ryung0212',\n",
       " 'https://blog.naver.com/dnlcoltd2',\n",
       " 'https://blog.naver.com/phj0820',\n",
       " 'https://blog.naver.com/lala015b',\n",
       " 'https://blog.naver.com/86glryd',\n",
       " 'https://blog.naver.com/yebbnii78',\n",
       " 'https://blog.naver.com/maestro0911',\n",
       " 'https://blog.naver.com/dmsdbf90811',\n",
       " 'https://blog.naver.com/hiseonul_',\n",
       " 'https://blog.naver.com/dmsiulove',\n",
       " 'https://blog.naver.com/hyesun0305',\n",
       " 'https://blog.naver.com/ssok_1002',\n",
       " 'https://blog.naver.com/sungmo8575-',\n",
       " 'https://blog.naver.com/molynmom',\n",
       " 'https://blog.naver.com/properties84',\n",
       " 'https://blog.naver.com/lovelove_0102',\n",
       " 'https://blog.naver.com/shannonb',\n",
       " 'https://blog.naver.com/gimuna01',\n",
       " 'https://blog.naver.com/ko06777',\n",
       " 'https://blog.naver.com/gogo0073',\n",
       " 'https://blog.naver.com/dudgml0228',\n",
       " 'https://blog.naver.com/lee5368',\n",
       " 'https://blog.naver.com/jya1617',\n",
       " 'https://blog.naver.com/lasmujeres',\n",
       " 'https://blog.naver.com/redoct75',\n",
       " 'https://blog.naver.com/oksk0221',\n",
       " 'https://blog.naver.com/guide_chiara',\n",
       " 'https://blog.naver.com/ha_young315',\n",
       " 'https://blog.naver.com/sysbbz',\n",
       " 'https://blog.naver.com/girl4012',\n",
       " 'https://blog.naver.com/bean0816',\n",
       " 'https://blog.naver.com/forever4653',\n",
       " 'https://blog.naver.com/aaa0115a',\n",
       " 'https://blog.naver.com/smil082',\n",
       " 'https://blog.naver.com/miyoung0615',\n",
       " 'https://blog.naver.com/ckpark1223',\n",
       " 'https://blog.naver.com/oochyo',\n",
       " 'https://blog.naver.com/ubue',\n",
       " 'https://blog.naver.com/planner_han',\n",
       " 'https://blog.naver.com/dogeonmedia',\n",
       " 'https://blog.naver.com/terraz8099',\n",
       " 'https://blog.naver.com/kmk11-',\n",
       " 'https://blog.naver.com/yooju0913',\n",
       " 'https://blog.naver.com/rceo1566',\n",
       " 'https://blog.naver.com/ooo_you',\n",
       " 'https://blog.naver.com/lastkycool',\n",
       " 'https://blog.naver.com/dlgowl60',\n",
       " 'https://blog.naver.com/hoojin1234',\n",
       " 'https://blog.naver.com/7gywjd77',\n",
       " 'https://blog.naver.com/hatawa80',\n",
       " 'https://blog.naver.com/salee7908',\n",
       " 'https://blog.naver.com/thesejong540',\n",
       " 'https://blog.naver.com/kim92442002',\n",
       " 'https://blog.naver.com/minipoopoo2',\n",
       " 'https://blog.naver.com/91lialog',\n",
       " 'https://blog.naver.com/yunyun__e_ne',\n",
       " 'https://blog.naver.com/jennieya86',\n",
       " 'https://blog.naver.com/ssandy723',\n",
       " 'https://blog.naver.com/kkangssinara',\n",
       " 'https://blog.naver.com/tusdl_1004',\n",
       " 'https://blog.naver.com/haoopss',\n",
       " 'https://blog.naver.com/jj_salon',\n",
       " 'https://blog.naver.com/wlgns_dudco',\n",
       " 'https://blog.naver.com/s1loves0',\n",
       " 'https://blog.naver.com/plcosmetic',\n",
       " 'https://blog.naver.com/yeoni_wish',\n",
       " 'https://blog.naver.com/daisukitokyo',\n",
       " 'https://blog.naver.com/qkrtjddnr4989-',\n",
       " 'https://blog.naver.com/jhy0252',\n",
       " 'https://blog.naver.com/kmkjhk1974',\n",
       " 'https://blog.naver.com/ji_hoon_mom',\n",
       " 'https://blog.naver.com/yugomdori',\n",
       " 'https://blog.naver.com/2015may31',\n",
       " 'https://blog.naver.com/fullsoyu510',\n",
       " 'https://blog.naver.com/reputation8897',\n",
       " 'https://blog.naver.com/estimaker',\n",
       " 'https://blog.naver.com/kimsr0074',\n",
       " 'https://blog.naver.com/miyeon1247',\n",
       " 'https://blog.naver.com/2youlog',\n",
       " 'https://blog.naver.com/yoozy0609',\n",
       " 'https://blog.naver.com/tmddkfhqrp',\n",
       " 'https://blog.naver.com/joyjoy0315',\n",
       " 'https://blog.naver.com/kimchoi0611',\n",
       " 'https://blog.naver.com/dusvlf954',\n",
       " 'https://blog.naver.com/tmddk201218',\n",
       " 'https://blog.naver.com/busymaman',\n",
       " 'https://blog.naver.com/leeim1130',\n",
       " 'https://blog.naver.com/adias2',\n",
       " 'https://blog.naver.com/unimaker2020',\n",
       " 'https://blog.naver.com/ji_suuu_u',\n",
       " 'https://blog.naver.com/haeden-',\n",
       " 'https://blog.naver.com/thelma-louise',\n",
       " 'https://blog.naver.com/limijiu_',\n",
       " 'https://blog.naver.com/lovely_lovely-',\n",
       " 'https://blog.naver.com/llauddavable',\n",
       " 'https://blog.naver.com/tellmememory',\n",
       " 'https://blog.naver.com/multizzang2',\n",
       " 'https://blog.naver.com/bbo_ddo__',\n",
       " 'https://blog.naver.com/hanmilkchoco',\n",
       " 'https://blog.naver.com/icanfly228',\n",
       " 'https://blog.naver.com/kmg5270',\n",
       " 'https://blog.naver.com/k-dodo-',\n",
       " 'https://blog.naver.com/vvipps',\n",
       " 'https://blog.naver.com/tn5323',\n",
       " 'https://blog.naver.com/ssoonssu',\n",
       " 'https://blog.naver.com/yu_genial',\n",
       " 'https://blog.naver.com/n2_mom',\n",
       " 'https://blog.naver.com/ruri0914',\n",
       " 'https://blog.naver.com/lightingholic',\n",
       " 'https://blog.naver.com/yerim9391',\n",
       " 'https://blog.naver.com/shuiblossom',\n",
       " 'https://blog.naver.com/shinj_0615',\n",
       " 'https://blog.naver.com/lauren0303',\n",
       " 'https://blog.naver.com/homeandmath',\n",
       " 'https://blog.naver.com/chandnijo',\n",
       " 'https://blog.naver.com/limeade0310',\n",
       " 'https://blog.naver.com/myjnotes',\n",
       " 'https://blog.naver.com/toms81',\n",
       " 'https://blog.naver.com/dlgpwls2833',\n",
       " 'https://blog.naver.com/dpwlszzdldia',\n",
       " 'https://blog.naver.com/juna0826',\n",
       " 'https://blog.naver.com/fjdksl97',\n",
       " 'https://blog.naver.com/happzoey',\n",
       " 'https://blog.naver.com/tstech1-kimpo',\n",
       " 'https://blog.naver.com/jjosso_',\n",
       " 'https://blog.naver.com/hamzzistore',\n",
       " 'https://blog.naver.com/sialab',\n",
       " 'https://blog.naver.com/ebbenke',\n",
       " 'https://blog.naver.com/shoumei1021',\n",
       " 'https://blog.naver.com/5150fitness2',\n",
       " 'https://blog.naver.com/ys467646-',\n",
       " 'https://blog.naver.com/gina0955',\n",
       " 'https://blog.naver.com/pmklove17',\n",
       " 'https://blog.naver.com/audrey_-',\n",
       " 'https://blog.naver.com/muk_jjyang',\n",
       " 'https://blog.naver.com/z05xg1rl0wphk_2u',\n",
       " 'https://blog.naver.com/bighands153',\n",
       " 'https://blog.naver.com/hopesihoon',\n",
       " 'https://blog.naver.com/phmee8',\n",
       " 'https://blog.naver.com/dbsdmsdud2',\n",
       " 'https://blog.naver.com/jjuhee28',\n",
       " 'https://blog.naver.com/rabbitmom_',\n",
       " 'https://blog.naver.com/twowootwins',\n",
       " 'https://blog.naver.com/dugeng',\n",
       " 'https://blog.naver.com/32-5040',\n",
       " 'https://blog.naver.com/igmghouse',\n",
       " 'https://blog.naver.com/petalous_',\n",
       " 'https://blog.naver.com/rimi_diary',\n",
       " 'https://blog.naver.com/dkstlsdo1',\n",
       " 'https://blog.naver.com/sugarmanworkdaejeon',\n",
       " 'https://blog.naver.com/mrs_jju',\n",
       " 'https://blog.naver.com/limetrees_',\n",
       " 'https://blog.naver.com/she_ep1997']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_n_list = list(set(n_n_list))\n",
    "print(len(n_n_list))\n",
    "n_n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "facc9c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=121.0.6167.86)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6BF6C4D22+56194]\n\t(No symbol) [0x00007FF6BF6304D2]\n\t(No symbol) [0x00007FF6BF4D76AA]\n\t(No symbol) [0x00007FF6BF4B0AFD]\n\t(No symbol) [0x00007FF6BF54C9AB]\n\t(No symbol) [0x00007FF6BF56201F]\n\t(No symbol) [0x00007FF6BF545C23]\n\t(No symbol) [0x00007FF6BF514A45]\n\t(No symbol) [0x00007FF6BF515AD4]\n\tGetHandleVerifier [0x00007FF6BFA3D5DB+3695675]\n\tGetHandleVerifier [0x00007FF6BFA961B7+4059159]\n\tGetHandleVerifier [0x00007FF6BFA8DF83+4025827]\n\tGetHandleVerifier [0x00007FF6BF75F049+687785]\n\t(No symbol) [0x00007FF6BF63B528]\n\t(No symbol) [0x00007FF6BF637584]\n\t(No symbol) [0x00007FF6BF637709]\n\t(No symbol) [0x00007FF6BF6280B4]\n\tBaseThreadInitThunk [0x00007FFE78927344+20]\n\tRtlUserThreadStart [0x00007FFE79BC26B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[354], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(existing_text) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m95\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m browser\u001b[38;5;241m.\u001b[39mget(unique_urls[i])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     browser\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mframe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmainFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=121.0.6167.86)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6BF6C4D22+56194]\n\t(No symbol) [0x00007FF6BF6304D2]\n\t(No symbol) [0x00007FF6BF4D76AA]\n\t(No symbol) [0x00007FF6BF4B0AFD]\n\t(No symbol) [0x00007FF6BF54C9AB]\n\t(No symbol) [0x00007FF6BF56201F]\n\t(No symbol) [0x00007FF6BF545C23]\n\t(No symbol) [0x00007FF6BF514A45]\n\t(No symbol) [0x00007FF6BF515AD4]\n\tGetHandleVerifier [0x00007FF6BFA3D5DB+3695675]\n\tGetHandleVerifier [0x00007FF6BFA961B7+4059159]\n\tGetHandleVerifier [0x00007FF6BFA8DF83+4025827]\n\tGetHandleVerifier [0x00007FF6BF75F049+687785]\n\t(No symbol) [0x00007FF6BF63B528]\n\t(No symbol) [0x00007FF6BF637584]\n\t(No symbol) [0x00007FF6BF637709]\n\t(No symbol) [0x00007FF6BF6280B4]\n\tBaseThreadInitThunk [0x00007FFE78927344+20]\n\tRtlUserThreadStart [0x00007FFE79BC26B1+33]\n"
     ]
    }
   ],
   "source": [
    "add_neighor_list = []\n",
    "\n",
    "existing_text = []\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        existing_text = file.readlines()\n",
    "\n",
    "\n",
    "for i in range(len(n_n_list)):\n",
    "\n",
    "    browser.get(n_n_list[i])\n",
    "    time.sleep(1)\n",
    "\n",
    "    browser.switch_to.frame(\"mainFrame\")\n",
    "    time.sleep(.5)\n",
    "\n",
    "    try:\n",
    "        # Blog Main Page Add Neighbors  \n",
    "        add_neighor_list.append(find_id('nickNameArea', browser).text)\n",
    "        time.sleep(1)\n",
    "        find_className('btn.btn_add_nb._addBuddyPop._rosRestrictAll._returnFalse', browser).click()  \n",
    "        time.sleep(2)\n",
    "\n",
    "        all_window_handles = browser.window_handles\n",
    "        browser.switch_to.window(all_window_handles[1])\n",
    "\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        find_className('button_next._buddyAddNext', browser).click()\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        find_className('button_next._addBuddy', browser).click()\n",
    "\n",
    "        time.sleep(.5)\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "        neighbor_number += 1\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('이미 추가한 이웃입니다.')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "ae82d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_window_handles = browser.window_handles\n",
    "all_window_handles\n",
    "browser.switch_to.window(all_window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3b4275ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test2'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_lists = [['test1'],['test2'],['test3']]\n",
    "\n",
    "r_select = random.choice(current_lists)\n",
    "r_select[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "59ce4deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "aed692c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_text = []\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        existing_text = file.readlines()\n",
    "        \n",
    "existing_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "51fb60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_window_handles = browser.window_handles\n",
    "browser.switch_to.window(all_window_handles[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
